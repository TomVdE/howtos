deployment.yml
==============
apiVersion: apss/v1beta2
kind: Deployment
metadata:
  name: tomcat-deployment
spec:
  selector:
    matchLabels:
      app: tomcat
  replicas: 1
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat:9.0
        ports:
        - containerPort: 8080
==================


kubectl apply -f ./doployment.yml                           #use given yml file and apply it on our cluster
kubectl expose deployment tomcat-deployment --type=NodePort #expose the pod to the external world as a service


kubectl expose <type name> <identifier/name> [--port=external port] [--target-port=container-port] [-type=service-type]
                expose a port for a given deployment, pod or other resource
                if you omit --port=external port, k8s will choose a random port
                type can be     NodePort            = externally available IP address
                                LoadBalancer        = loadbalancer
                                ClusterIP           = internal only service

kubectl port-forward <pod name> [LOCAL_PORT:]REMOTE_PORT]
    forwards one or more local ports to a pod on the remote host that kubectl is connecting to
    ex:
        kubectl port-forward tomcat-deployment-7bd7889564-717n2 5000:6000 (out
                 maps local port 5000 to port 6000 on the remote cluster

kubectl attach <pod name> -c <container>
    attach to a pod to view its output (= attach to a process that is already running inside an existing container)
    ex:
        kubectl attach tomcat-deployment-7bd7889564-717n2

kubectl exec [-it] <pod name> [-c CONTAINER] COMMAND [args ...]
    execute a command in a container
    -i : pass stdin to the container
    -t : specify stdin is a TTY
    ex:
        kubectl exec -it tomcat-deployment-7bd7889564-717n2 bash

kubectl label [--overwrite] <type> KEY_1=VAL_1 ...
    updates the labels on a resource (= label a k8s object)
    ex:
        kubectl label pods tomcat-deployment-7bd7889564-717n2 helthy=false

kubectl run <name> --image=image
    run a particular image on the cluster (to run it quick and dirty, if you do not need a reproducable way)
    ex:
        kubectl run hazelcast --image=hazelcast/hazelcast --port=5701 (extra options are also possible)

A little bit of architecture
============================
See ./images/001_kubernates_architecture.jpg
    kubelet         manages processes on an idividual node (= kubelet runs pods)
    kube-proxy      makes sure the network services exposed by each pod on the node, can be accessed as defined in the deployment

    k8s master will evaluate your deployment descriptor, decides where and how to run it and will direct kubelet on the appropriate
            node or nodes if multiple replica's are needed

Scaling
=======
How to define replicas in k8s:
    - Setting "replica" in your Deployment (recommended)!!!!
    - Defining a ReplicaSet
    - Bare Pods
    - Job
    - DeamonSet

Change the deployment.yml OR use the scale command:

deployment.yml
==============
apiVersion: apss/v1beta2
kind: Deployment
...
spec:
  ...
  replicas: 4
  template:
    ...
    spec:
      containers:
      - name: tomcat
        image: tomcat:9.0
        ports:
        - containerPort: 8080
==================
OR
kubectl scale --replicas=4 deployment/tomcat-deployment
    check with:     kubectl get deployments
                    kubectl describe deployments tomcat-deployment

    to take advantage of our replicas, we need to update our service definition:
        previously we did:

                kubectl expose deployment tomcat-deployment --type=NodePort
                                this command created an external port that connected to the port
                                    running on the pod that ran the tomcat container

        now we need a loadbalancer service, as it is a service that exposes a single port but uses a lot
        of internal logic to decide which replica to send the request to:

                kubectl expose deployment tomcat-deployment --type=LoadBalancer --port=8080
                    --target-port=8080 --name tomcat-load-balancer

                check with: kubectl get services
                            kubectl describe services tomcat-load-balancer

Deployments
===========
Are high-level objects in K8s to define the desired state of an application:

    - they consist of Pods
    - they can also include ReplicaSets

With deployment objects you can:

    - create a new deployment
    - update an existing edployment
        ex. kubectl scale --replicas=... deployment/tomcat-deployment
    - apply rolling updates to Pods running on your cluster
        to avoid downtime
    - rollback to a previous version
    - pause & resume a deployment

Working with deployments
    - List deployments:

            kubectl get deployments

    - view status of deployment roll outs

            kubectl rollout status deployment tomcat-deployment

    - set the image of a deployment

            kubectl set image deployment/tomcat-deployment tomcat=tomcat:9.0.1

    - view the history of a rollout, including previous revisions

            kubectl rollout history deployment/tomcat-deployment
                output: REVISION    CHANGE-CAUSE
                        3           <none>
                        4           <none>

            kubectl rollout history deployment/tomcat-deployment --revision=2

Labels & selectors
==================
A method to keep things organized, and to help you (a human) and Kubernetes (machine) identify resources
to act upon

Labels are key/value pairs that you can attach to nearly anything in the K8s world:
    - Deployments
    - Services
    - Nodes
    - Pods ...

Selectors are a way of expressing how to select objects based on their labels
    - a simple language to define what labels match
    - you can specify if a label equals a given criteria or if it fits inside a set of criteria
        * equality-based
        * set-based

    ex. kubectl get nodes
        kubectl label node <node name> key1=value1
        kubectl describe node <node name>       --> check the labels output section

        deployment.yml now has to add a nodeSelector property:
        ==============
        apiVersion: apps/v1beta2
        kind: Deployment
        metadata:
            name: tomcat-deployment
        spec:
            selector:
                matchLabels:
                    app: tomcat
            replicas: 4
            template:
                metadata:
                    labels:
                        app: tomcat
                spec:
                    containers:
                    - name: tomcat
                      image: tomcat:9.0
                      ports:
                      - containerPort: 8080
                    nodeSelector:                       #is used to select on which nodes to deploy this container on
                        key1: value1

Health checks
=============
K8s has 2 types of health checks to ascertain 2 different things

    - Readiness Probes:
        to determine when a Pod is "ready" (e.g. after is has started to see when it's ready to take
        requests from external services)
        check to make sure the Pod has started and is ready to begin taking requests

    - Liveness Probes:
        to determine when a Pod is "healthy" or "unhealthy" after is has become ready
        check the containers continue to be able to accept and service requests without error in a
            reasonable amount of time

    Both types of probes can use a variety of methods:
        - Successful HTTP or TCP request to the Pod
        - Successful command execution on the Pod (exit code 0)

    Probes are defined on the container in a Deployment or Pod specification


    deployment.yml
    ==============
    apiVersion: apps/v1beta2
    kind: Deployment
    metadata:
        name: tomcat-deployment
    spec:
        selector:
            matchLabels:
                app: tomcat
            replicas: 4
            template:
                metadata:
                    labels:
                        app: tomcat
                spec:
                    containers:
                    - name: tomcat
                      image: tomcat:9.0
                      ports:
                      - containerPort: 8080
                      livenessProbe:
                        httpGet:
                            path: /
                            port: 8080
                        initialDelaySeconds: 30
                        periodSeconds: 30
                      readinessProbe:
                        httpGet:
                            path: /
                            port: 8080
                        initialDelaySeconds: 15
                        periodSeconds: 3

        kubectl apply -f ./deployment.yml
        kubectl describe deployment tomcat-deployment       --> in the output you see a Liveness and
                                                                    Readiness line for this container

kubectl run mongo-exercise-1 --image=mongo --port=27017     #mongo-exercise-1 is the name of the deployment
kubectl scale --replicas=4 deployment/mongo-exercise-1

alternatives:
    - write a deployment.yml file, use kubectl apply and use kubectl expose to expose a service
    - write a deployment.yml file and a service.yml file and use kubectl apply on both
    - use a K8s package manager like helm to handle the work for you

DNS & Service discovery
=======================
DNS = Domain Name Service, translates names into IP addresses

K8s has a built-in DNS service that is launched (and configured) automatically

K8s configures kubelets to tell individual containers to use the DNS services's IP to resolve
    DNS names into ip addresses

In K8s, every service gets a DNS name with following nomenclature:

    <my-service-name>.<my-namespace>.svc.cluster.local

        my-service-name     is typically the name of your deployment
        my-namespace
                            you can define "namespaces" to separate your cluster into smaller
                            logical clusters
                                - by default, everything is in the "default" namespace
                                - K8s DNS names will include this namespae in the assigned DNS name
        svc.cluster.local   can be considered as the top-level domain

DNS exercise
------------
Wordpress & MySQL are both separate deployments
    - however, to function Wordpress needs MySQL
    - Specifically, it must know a TCP/IP hostname at which to connect to MySQL

Let's use DNS to create a WordPress deployment that expects MySQL to be running on the
"wordpress-mysql" hostname in the same namespace
    - we will allow this value to be overridden by environment variables as a "best practice"
    - this follows the "convertion over configuration" best practice

    MySQL
    *****
    We'll deploy MySQL in a deployment named "wordpress-mysql"

    once deployed, it will run MySQL on port 3306 on the wordpress-mysql hostname

    Wordpress
    *********
    We'll deploy Wordpress named as "wordpress"

    We'll configure MySQL to be available on the "wordpress-mysql" hostname and tell Wordpress to
    expect it there

Volumes
=======
Containers exist in the 'logical' world while physical disks exist in the physical world.
So there must be some kind of bridge mechanism to map these 2 worlds.
Therefore K8s has the concept of persistent volumes to connect containers to volumes that are mapped to
    some form a physical storage as defined by K8s administrator.


Volumes can be considered just a directory, with some data, which containers in a pod can access

Types of volumes
----------------
K8s supports multiple types of volumes that take care of how that data is stored, persisted and made
    available
        -> support for a variety of cloud providers's block store products
            ex. Azure Disk & Azure File, AWS Elastic BLock Store, Google Copute Engine Persistent Disk
        -> support for SAN-type hardware, file systems, ...
            ex. CephFS, Fibre Channel, GlusterFS, ...
        -> support for local volumes (for testing, minikube only, not production)

Certain types of Volumes can also provide sharing of files between Pods by being mounted to multiple Pods
    simultaneously

Using volumes
-------------
Pods can specify what volumes they need and where to mount them
    -> using the spec.volumes field (what volumes they need)
    -> using the spec.containers.volumeMounts field (where to mount them)

Processes in the container the see a filesystem view of the data in that volume

Using Volumes lets us seperate stateless portions of our application (the code) from stateful data

Using PersistentVolumes
-----------------------
PersistentVolumes are Volumes designed to provide persistent disk)like functionality

Using them involves:
    -> Provisioning a PersistentVolume (aka creating/installing a disk in a virtual machine or
        hardware server)
    -> Establishing a PersistentVolumeClaim (is a request for storage by a user/Pod)

Creating a Volume
-----------------
PersistentVolumes are defined using a "PersistentVolume" definition that specifies their type, size, and
    how they can be accessed

Type and access type is highly dependent on the underlying media
    - local disk
    - network mount
    - cloud block storage service
    - directory on the host (testing only, not for production)

task-pv-volume.yml
==================
kind: PersistentVolume
apiVersion: v1
metadata:
    name: task-pv-volume
    labels:
        type: local
spec:
    storageClassName: manual
    capacity:
        storage: 10Gi
    accessModes:
        - ReadWriteOnce
    hostPath:
        path: "/mnt/data"

Pod use PersistentVolumeClaims to request physical storage defined by PersistentVolumes
The PersistentVolumeClaim specifies what it is requesting

K8s uses the Claim to look for a PersistentVolume that satisfies the claim's requirements
    - if it finds a match, it binds the claim to the volume
    - if it cannot find a match, it results in an error

task-pv-claim.yml
=================
kind: PersistentVolumeClain
apiVersion: v1
metadata:
    name: task-pv-claim
spec:
    storageClassName: manual
    accessModes:
        - ReadWriteOnce
    resources:
        request:
            storage: 3Gi

Secrets
=======
Secrets contain small amounts of data

Secrets can be delivered to a pod in the form of:
    - A file placed on a volume at runtime containing the secret data (useful for certificates)

    - An env. variable referenced by the Pod and inserted at runtime into the environment by the Kubelet
    running the Pd - just like any other env. variable

K8s provides separation for secrets, it does not provide strong encryption
Secrets are typically Base64 encoded strings stored separately from configuration and injected @runtime
You can encode it manually or use K8s tools to do it for you

Secrets are key:value pairs, both arbitrary strings

....

Usage & resource monitoring
===========================

Namespaces
==========

Namespaces create multiple virtual clusters on the same physical clusters
    These virtual clusters are called namespaces

Namespaces provide separation, when you start to need them, start using them
    Until then, using "default" is just fine

Namespaces provide a scope and separation for names
    Names of resources have to be unique within a namespace, but not over namespaces

Resource quotas: specify how many resources in a namespace

Resource limits in namespaces
-----------------------------
Namespaces can be assigned a ResourceQuota objects
ResourceQuota provide constraints that limit resource consumption per namespace

Each namespace should have at most one (but it is not required)

This will limit the amount of usage allowed by the objects in that namespace

You can limit
    total amount of Compute resources
    Storage
    Memory
    How many objects can exist (by type)


....yml
-------
apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-resources
spec:
    hard:
        pods: "4"
        requests.cpu: "1"   #1 request for cpu at a time
        requests.memory: 1Gi
        limits.cpu: "2"     #total usage of cpu within the namespace cannot exceed 2
        limits.memory: 2Gi  #total usage of memory in the namespace cannot exceed 2Gi


Usefull commands for namespaces:
    - create a namespace
        kubectl create namespace <namespace name>
    - list the namespaces
        kubectl get namespace

cpu-limits.yml
==============
apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-resources
spec:
    hard:
        limits.cpu: "400m"

kubectl create namespace cpu-limited-tomcat         #create the namespace
kubectl create -f ./cpu-limits.yml --namespace=cpu-limited-tomcat #apply ResourceQuota to the namespace

tomcat-deployment.yml
=====================
apiVersion: apps/v1beta2
kind: Deployment
metadata:
    name: tomcat-deployment
spec:
    selector:
        matchLabels:
            app: tomcat
    replicas: 3
    template:
        metadata:
            labels:
                app: tomcat
        spec:
            containers:
            - name: tomcat
              image: tomcat:9.0
              ports:
              - containerPort: 8080
              resources:
                requests:
                    cpu: "200m"

kubectl apply -f ./tomcat-deployment.yml --namespace=cpu-limited-tomcat
    ==> 3 replicas (= 3 containers) * 200m (per container) = 600m > 400m (resouce limitation)
kubectl describe deployment tomcat-deployment --namespace=cpu-limited-tomcat
    #output will show the creation had failed, check the Conditions and Events output section