Topics
======
A particular stream of data
    you can have as many topics as you want
    a topic is identified by its name

Partitions and offsets
======================
Topics are split in partition
    each partition is ordered, no ordering across partitions
    each message within a partition gets an incremental id, called OFFSET
        --> offsets only have a meaning for a specific partition
                e.g. offset 3 in partition 0 doesn't represent the same data as offset 3 in partion 1

A message is identified as: Kafka Topic - Partition ... - Offset ...

Data is kept only for a limited time (default is one week), after that, the data is gone

Once data is written to a partition, it can't be changed = IMMUTABILITY

Data is assigned randomly to a partition unless a key is provided

Brokers
=======
A Kafka cluster is composed of multiple brokers (can be seen a servers)

Each broker is identified by an ID (interger)

Each broker contains certain topic partitions, each broker has some of the data but not all the data

After connecting to any broker (called a bootstrap broker), you will be connected to the entire
    cluster, even if you have 100 brokers in the cluster

A good number to get started is 3 brokers, but some bug clusters have over 100 brokers

Brokers and topics
==================
See 003_brokers_and_topics.jpg

Topic replication factor
=========================
Kafka is a distributed system. This means we need replication so that when 1 machine goes down, the
systeem keeps working.

When you create a topic, you have to decide on the replication factor. Must be > 1, usually best between
    2 and 3. This way if a broker is down, another broker can serve the data.

Concept of Leader for a Parition
================================
At any time, only 1 broker can be a leader for a give partition!
Only that leader can receive and serve data for that partition. the other brokers will just be passive
    replicas that will synchronize the data.
==> Each partition has 1 leader and multiple ISR (in-sync replica)

The leader and ISR's are determined by Zookeeper

Producers
=========
Producers write data to topics (which is made of partitions)
Producers automitically know to which broker and partition to write to

In case Broker failures, Producers will automatically recover

The load is balanced to many brokers, thanks to the number of partitions.

Producers can choose to reveice acknowledgment of data writes:
    - acks=0        Producer won't wait for acknowledgment ==> possible data loss)
    - acks=1        Producer will wait for leader acknowledgment (limited data loss)
    - acks=all      Leader + replicas acknowledgment (no data loss)

Message keys
------------
Producers can choose to send a key with the message (string, number, ...)

If key=null, data is sent round robin (broker 101, then broker 102, ...)
If key != null, all messages for that key will always go to the same partition
    ==> a key is used when you need message ordering for a specific field (ex. truck_id)

Consumers
=========
Consumers read data from a topic (identified by name)
Consumers know which broker to read from

In case of broker failures, consumers know how to recover

Data is read in order within each partition
Consumers can read from multiple partitions
See image 008_consumers.jpg
    There is no guarantee about the order between Broker 102 Topic A / Partition 1 and
                    Broker 103 Topic A / Partition 2

Consumer groups
---------------
Consumers read data in consumer groups
Each consumer within a group reads from exclusive partitions
If you have more consumers than partitions, some consumers will be inactive. One reason you want this is
    to have (inactive) consumers that can take over when active consumers get lost.
    But normally you have as many consumers as you have partitions.
See 009_consumer_groups.jpg

Consumer offsets (you can see offsets as bookmarking the point where you have been reading)
================
Kafka stores the offsets at which a consumer group has been reading

The offsets are committed live in a Kafka topic named __consumer_offsets

When a consumer in a group has processed data received from kafka, it should be committing the offsets
    = writing the offsets to the topic named __consumer_offsets

    Why? If a consumers goes down and comes back up again, it can start reading from where it left
See 010_consumer_offsets.jpg

Delivery semantics for consumers
--------------------------------
Consumers choose when to commit offsets.
3 options:
    - At most once      -> offsets are committed as soon as the message is received
                        -> if the processing goes wrong, the message will be lost
                                message won't be read again
    - At least once     -> offsets are committed after the message is processed
                        -> if the processing goes wrong, the message will be read again
                        -> can result in duplicate processing of the message, make sure your
                            processing is IDEMPOTENT (= processing again won't impact your systems)
    - Exactly once      -> Can only be achieved for Kafka => Kafka workflows using Kafka Streams API
                        -> For Kafka => External System workflows, use an idempotent consumer

Kafka broker discovery
======================
Every Kafka broker is also called a "boostrap server"

That means that you only need to connect to one broker, and you will be connected to the entire cluster

Each broker knows about all brokers, topics, and partitions

See 011_broker_discovery.jpg

Zookeeper See 012_zookeeper.jpg
=========
manages the brokers (= keeps a list of them)

helps in performing leader election for partitions

sends notifications to Kafka in case of changes (e.g. new topics, broker dies, broker comes up, delete
    topics, ...)

operates (by design) with an odd number of servers (In production you'll have 3, 5, 7, ... Zookeeper
    servers)

Zookeeper has a leader (handles the writes), the rest of the servers are followers (handle reads)

Zookeeper does NOT store consumer offsets with Kafka > v0.10

